{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MLP Predictor for stock data</h1>\n",
    "\n",
    "<h3>Author: Danilo Filippo Reiszel Pereira</h3>\n",
    "<strong>Univesidade Federal do Rio de Janeiro</strong>\n",
    "\n",
    "<p>This notebook contains the creation a Multi-layer Perceptron model to predict D+1 closing prices of stocks</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append('../src')\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from neuralstocks.dataacquisition import *\n",
    "from neuralstocks.plots import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Global Params</h3>\n",
    "\n",
    "Setting global params such as directory of data, save directories and plot params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saveVarDir already exists!\n",
      "saveFigDir already exists!\n",
      "saveModelDir created\n"
     ]
    }
   ],
   "source": [
    "dataPath = '../../data'\n",
    "assetType = 'stocks'\n",
    "asset = 'PETR4'\n",
    "frequency = 'diario'\n",
    "filePath = dataPath + '/' + assetType + '/' + asset + '/' + frequency + '/' + asset + '.CSV'\n",
    "\n",
    "pathPETR4 = '../../data/stocks/PETR4/diario/PETR4.CSV'\n",
    "pathIBOV = '../../data/indexes/IBOV/diario/IBOV.CSV'\n",
    "pathUSDBRL = '../../data/forex/USDBRL/diario/USDBRL.CSV'\n",
    "pathEURBRL = '../../data/forex/EURBRL/diario/EURBRL.CSV'\n",
    "\n",
    "decomposeModel = 'additive'\n",
    "\n",
    "saveVarDir = './Variables'\n",
    "if not os.path.exists(saveVarDir):\n",
    "    os.makedirs(saveVarDir)\n",
    "    print('saveVarDir created')\n",
    "else:\n",
    "    print('saveVarDir already exists!')\n",
    "saveFigDir = './Figures'\n",
    "if not os.path.exists(saveFigDir):\n",
    "    os.makedirs(saveFigDir)\n",
    "    print('saveFigDir created')\n",
    "else:\n",
    "    print('saveFigDir already exists!')\n",
    "saveModelDir = './Models'\n",
    "if not os.path.exists(saveModelDir):\n",
    "    os.makedirs(saveModelDir)\n",
    "    print('saveModelDir created')\n",
    "else:\n",
    "    print('saveModelDir already exists!')\n",
    "saveFormat = 'png'\n",
    "\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['figure.titlesize'] = 18\n",
    "plt.rcParams['figure.titleweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 15\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['axes.labelsize'] = 15\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 13\n",
    "plt.rcParams['ytick.labelsize'] = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Acquisition and Preprocessing</h3>\n",
    "\n",
    "Gathers and preprocess data. Data from auxiliary time series (such as IBOV index and USDBRL exchange rate) is also gathered, though tipically less indicators are calculated for this series. Trend Extraction is applied to desired series.\n",
    "\n",
    "<strong>List of indicators applied to main time series: </strong>\n",
    "<ul>\n",
    "    <li>Mean and Standard Deviation, 20 periods</li>\n",
    "    <li>Returns Calculations:\n",
    "        <ul>\n",
    "            <li>Close<sub>d</sub> / Close<sub>d-1</sub></li>\n",
    "            <li>Close<sub>d</sub> / Open<sub>d</sub></li>\n",
    "            <li>High<sub>d</sub> / Close<sub>d</sub></li>\n",
    "            <li>Low<sub>d</sub> / Close<sub>d</sub></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>EMAs:\n",
    "        <ul>\n",
    "            <li>Close, 17 periods</li>\n",
    "            <li>Close, 72 periods</li>\n",
    "            <li>Close, 200 periods</li>\n",
    "            <li>Volume, 21 periods</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>MACD, fast EMA = 12 periods, slow EMA = 26 periods, signal EMA = 9 periods</li>\n",
    "    <li>Bollinger Bands, 20 periods</li>\n",
    "    <li>On-Balance Volume (OBV)</li>\n",
    "</ul>\n",
    "\n",
    "<strong>Parameters for trend extraction:</strong>\n",
    "<ul>\n",
    "    <li>Column: Close</li>\n",
    "    <li>Decompose model: Additive</li>\n",
    "    <li>Fitting Order: 1 (by default)</li>\n",
    "    <li>Window Size: 6</li>\n",
    "    <li>Fitting Weigths Model: Autocorrelation</li>\n",
    "    <li>Fitting Weigths Model Window Size: 18</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to acquire and process data: 11.47757411 seconds\n"
     ]
    }
   ],
   "source": [
    "init_time = time.time()\n",
    "PETR4 = acquireData(filePath = pathPETR4,\n",
    "                    replicateForHolidays = True,\n",
    "                    meanStdLen = 20,\n",
    "                    returnCalcParams = [['Close'], \n",
    "                                        ['Close', 'Open'],\n",
    "                                        ['High', 'Close' ],\n",
    "                                        ['Low', 'Close']],\n",
    "                    EMAparams = [{'column': 'Close', 'lenght': 17}, \n",
    "                                 {'column': 'Close', 'lenght': 72}, \n",
    "                                 {'column': 'Close', 'lenght': 200}, \n",
    "                                 {'column': 'Volume', 'lenght': 21}],\n",
    "                    MACDParams = [{'fast_lenght': 12, 'slow_lenght': 26, 'signal_lenght': 9}],\n",
    "                    BBParams = [{'lenght': 20}],\n",
    "                    OBVParams = [{'lenght': None}],\n",
    "                    deTrendParams = {'column': 'Close', 'window': 6, 'model': decomposeModel, \n",
    "                                     'weightModel': 'window_acorr', 'weightModelWindow': 18},\n",
    "                    colPrefix = 'PETR4',\n",
    "                    dropNan = True\n",
    "                   )\n",
    "\n",
    "IBOV = acquireData(filePath = pathIBOV,\n",
    "                    replicateForHolidays = True,\n",
    "                    meanStdLen = 20,\n",
    "                    returnCalcParams = [['Close'], \n",
    "                                        ['Close', 'Open'],\n",
    "                                        ['High', 'Close' ],\n",
    "                                        ['Low', 'Close']],\n",
    "                    EMAparams = [{'column': 'Close', 'lenght': 17}, \n",
    "                                 {'column': 'Close', 'lenght': 72}, \n",
    "                                 {'column': 'Close', 'lenght': 200}],\n",
    "                    colPrefix = 'IBOV',\n",
    "                    dropNan = True\n",
    "                  )\n",
    "\n",
    "USDBRL= acquireData(filePath = pathUSDBRL,\n",
    "                    replicateForHolidays = True,\n",
    "                    meanStdLen = 20,\n",
    "                    returnCalcParams = [['Close'], \n",
    "                                        ['Close', 'Open'],\n",
    "                                        ['High', 'Close' ],\n",
    "                                        ['Low', 'Close']],\n",
    "                    EMAparams = [{'column': 'Close', 'lenght': 17}, \n",
    "                                 {'column': 'Close', 'lenght': 72}, \n",
    "                                 {'column': 'Close', 'lenght': 200}],\n",
    "                    colPrefix = 'USDBRL',\n",
    "                    dropNan = True\n",
    "                   )\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print 'Time to acquire and process data: '+str(end_time-init_time)+' seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset concatenation and selection of wich features to feed to the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([PETR4, IBOV, USDBRL], axis = 1).dropna()\n",
    "\n",
    "columnsToUse = ['PETR4_Close_resid', \n",
    "                'PETR4_Close_rollStd20', \n",
    "                'PETR4_Close_returns', 'PETR4_Close/Open_returns', 'PETR4_High/Close_returns', 'PETR4_Low/Close_returns', \n",
    "                'PETR4_Close_EMA17_logdiff', 'PETR4_Close_EMA72_logdiff', 'PETR4_Close_EMA200_logdiff', 'PETR4_Volume_EMA21_logdiff', \n",
    "                'PETR4_MACD_12_26_9', 'PETR4_MACDsignal_12_26_9', 'PETR4_Bollinger%b_20', 'PETR4_OBV', \n",
    "                'PETR4_Holiday',\n",
    "                'IBOV_Close_rollStd20', \n",
    "                'IBOV_Close_returns', 'IBOV_Close/Open_returns', 'IBOV_High/Close_returns', 'IBOV_Low/Close_returns',\n",
    "                'IBOV_Close_EMA17_logdiff', 'IBOV_Close_EMA72_logdiff', 'IBOV_Close_EMA200_logdiff',\n",
    "                'USDBRL_Close_rollStd20', \n",
    "                'USDBRL_Close_returns', 'USDBRL_Close/Open_returns', 'USDBRL_High/Close_returns', 'USDBRL_Low/Close_returns',\n",
    "                'USDBRL_Close_EMA17_logdiff', 'USDBRL_Close_EMA72_logdiff', 'USDBRL_Close_EMA200_logdiff',\n",
    "               ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creation of feature matrix and output array, split of data in Train and Test sets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches: 4185\n",
      "Number of test batches: 175\n",
      "Number of inputs per batch: 40\n"
     ]
    }
   ],
   "source": [
    "xTrain, yTrain, xTest, yTest = prepData(df = df, \n",
    "                                        columnsToUse = columnsToUse, columnToPredict = 'PETR4_Close_resid', \n",
    "                                        nDelays = 10, testSetSize = len(df['2017'])\n",
    "                                       )\n",
    "\n",
    "numberOfTrainBatches = len(xTrain)\n",
    "numberOTestBatches = len(xTest)\n",
    "numberOfTrainInputs = len(xTrain[0])\n",
    "print 'Number of train batches: {}'.format(numberOfTrainBatches)\n",
    "print 'Number of test batches: {}'.format(numberOTestBatches)\n",
    "print 'Number of inputs per batch: {}'.format(numberOfTrainInputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Scalling</h3>\n",
    "\n",
    "Technique used: Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xNormScaler = MinMaxScaler(feature_range = (-1,1))\n",
    "xNormScaler.fit(xTrain)\n",
    "\n",
    "yNormScaler = MinMaxScaler(feature_range = (-1,1))\n",
    "yNormScaler.fit(yTrain)\n",
    "\n",
    "xNormTrain = xNormScaler.transform(xTrain)\n",
    "xNormTest  = xNormScaler.transform(xTest)\n",
    "\n",
    "yNormTrain = yNormScaler.transform(yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Parameters for traning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = xTrain.shape[1]\n",
    "\n",
    "nFolds = 10 # number of times a topology will be trained\n",
    "maxNeuronsInHiddenLayer = 40 # upper limit for number of neurons in hidden layer\n",
    "\n",
    "# optimizer definition\n",
    "optimizer = optimizers.SGD(lr=0.003, momentum=0.00, decay=0.0, nesterov=False)\n",
    "\n",
    "# callbacks definitions\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=25, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training of MLP model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "Unable to create file (unable to open file: name = 'Models/PETR4_MLP_norm.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-263511fcec08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                                \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                callbacks = [modelCheckpoint,\n\u001b[0;32m---> 22\u001b[0;31m                                             earlyStopping])\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitHistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbestNormValLoss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/danilo/.virtualenv/sat/tsa/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/danilo/.virtualenv/sat/tsa/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/home/danilo/.virtualenv/sat/tsa/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1201\u001b[0m                             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                                 \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/danilo/.virtualenv/sat/tsa/lib/python2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/danilo/.virtualenv/sat/tsa/lib/python2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    415\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/danilo/.virtualenv/sat/tsa/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   2551\u001b[0m         \"\"\"\n\u001b[1;32m   2552\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2553\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/danilo/.virtualenv/sat/tsa/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keras_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'backend'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/danilo/.virtualenv/sat/tsa/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/danilo/.virtualenv/sat/tsa/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: Unable to create file (unable to open file: name = 'Models/PETR4_MLP_norm.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "modelCheckpoint = ModelCheckpoint('./Models/PETR4_MLP_norm.h5', save_best_only=True)\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "bestNormValLoss = np.Inf\n",
    "bestNormFitHistory = None\n",
    "\n",
    "for nNeurons in range(1, maxNeuronsInHiddenLayer + 1):\n",
    "    for fold in range(nFolds):\n",
    "        model = Sequential([Dense(nNeurons, activation = 'tanh', input_dim = inputDim),\n",
    "                            Dense(1, activation = 'linear')\n",
    "                           ])\n",
    "        model.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['mae'])\n",
    "\n",
    "        fitHistory = model.fit(xNormTrain, \n",
    "                               yNormTrain, \n",
    "                               epochs = 500,\n",
    "                               verbose = 0,\n",
    "                               shuffle = True,\n",
    "                               validation_split = 0.15, \n",
    "                               callbacks = [modelCheckpoint,\n",
    "                                            earlyStopping])\n",
    "        \n",
    "        if min(fitHistory.history['val_loss']) < bestNormValLoss:\n",
    "            bestNormValLoss = min(fitHistory.history['val_loss'])\n",
    "            bestNormFitHistory = fitHistory\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print 'Time to fit all models: '+str(end_time-init_time)+' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNormModel = load_model('./Models/PETR4_MLP_norm.h5')\n",
    "bestNormModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(bestNormModel).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot of RMSE per epoch, for Training Set and Validation Set</h3>\n",
    "Note: Validation set is not the same as Test Set; it's randomly generated from the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10), nrows = 1, ncols = 1)\n",
    "ax.set_title('RMSE per epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('RMSE')\n",
    "trainigSet, = ax.plot(np.sqrt(bestNormFitHistory.history['loss']), 'b', label = 'trainig set')\n",
    "validationSet, = ax.plot(np.sqrt(bestNormFitHistory.history['val_loss']), 'r', label = 'validation set')\n",
    "plt.legend(handles=[trainigSet, validationSet], labels=['training set', 'validation set'], prop={'size': 15})\n",
    "plt.figtext(0.5,  0.010, 'Lowest Validation RMSE: {}'.format(min(bestNormFitHistory.history['val_loss'])), size = 14, horizontalalignment = 'center')\n",
    "fig.savefig('{}/{}.{}'.format(saveFigDir, 'PETR4_MLP_norm', 'pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot of predictions made using the Test Set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsNorm = bestNormModel.predict(xNormTest)\n",
    "predictionsNorm = yNormScaler.inverse_transform(predictionsNorm)\n",
    "predictionsNorm_clean = []\n",
    "for p in predictionsNorm:\n",
    "    predictionsNorm_clean.extend(p)\n",
    "predictionsNormSeries = pd.Series(predictionsNorm_clean, index = df['2017'].index)\n",
    "\n",
    "plotSeries([df['PETR4_Close_trend']+ df['PETR4_Close_resid'], \n",
    "#            df['PETR4_Close_trend'],\n",
    "            df['PETR4_Close_trend'] + predictionsNormSeries],\n",
    "           initialPlotDate = '2017-05', finalPlotDate = '2017-06',\n",
    "           title = 'Original Data vs Predicted', ylabel = 'Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot of errors from Trend Prediction and Trend + ANN Prediction</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSeries([df['PETR4_Close_resid'], (df['PETR4_Close_resid'] - predictionsNormSeries)],\n",
    "           plotZeroLine = True,\n",
    "           initialPlotDate = '2017-05', finalPlotDate = '2017-06',\n",
    "           title = 'Prediction Error', ylabel = 'BRL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Retraining same topologies, but using standardization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "xStdScaler = StandardScaler()\n",
    "xStdScaler.fit(xTrain)\n",
    "\n",
    "yStdScaler = StandardScaler()\n",
    "yStdScaler.fit(yTrain)\n",
    "\n",
    "xStdTrain = xStdScaler.transform(xTrain)\n",
    "xStdTest  = xStdScaler.transform(xTest)\n",
    "\n",
    "yStdTrain = yStdScaler.transform(yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCheckpointStd = ModelCheckpoint('./Models/PETR4_MLP_std.h5', save_best_only=True)\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "bestStdValLoss = np.Inf\n",
    "bestStdFitHistory = None\n",
    "\n",
    "for nNeurons in range(1, maxNeuronsInHiddenLayer + 1):\n",
    "    for fold in range(nFolds):\n",
    "        modelStd = Sequential([Dense(nNeurons, activation = 'tanh', input_dim = inputDim),\n",
    "                               Dense(1, activation = 'linear')\n",
    "                              ])\n",
    "        modelStd.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['mae'])\n",
    "\n",
    "        fitHistory = modelStd.fit(xStdTrain, \n",
    "                                  yStdTrain, \n",
    "                                  epochs = 500,\n",
    "                                  verbose = 0,\n",
    "                                  shuffle = True,\n",
    "                                  validation_split = 0.15, \n",
    "                                  callbacks = [modelCheckpointStd,\n",
    "                                               earlyStopping])\n",
    "        \n",
    "        if min(fitHistory.history['val_loss']) < bestStdValLoss:\n",
    "            bestStdValLoss = min(fitHistory.history['val_loss'])\n",
    "            bestStdFitHistory = fitHistory\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print 'Time to fit all models: '+str(end_time-init_time)+' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bestModelStd = load_model('./Models/PETR4_MLP_std.h5')\n",
    "bestModelStd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10), nrows = 1, ncols = 1)\n",
    "ax.set_title('RMSE per epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('RMSE')\n",
    "trainigSet, = ax.plot(np.sqrt(bestStdFitHistory.history['loss']), 'b', label = 'trainig set')\n",
    "validationSet, = ax.plot(np.sqrt(bestStdFitHistory.history['val_loss']), 'r', label = 'validation set')\n",
    "plt.legend(handles=[trainigSet, validationSet], labels=['training set', 'validation set'], prop={'size': 15})\n",
    "plt.figtext(0.5,  0.010, 'Lowest Validation RMSE: {}'.format(min(bestStdFitHistory.history['val_loss'])), size = 14, horizontalalignment = 'center')\n",
    "fig.savefig('{}/{}.{}'.format(saveFigDir, 'PETR4_MLP_std', 'pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsStd = bestModelStd.predict(xStdTest)\n",
    "predictionsStd = yStdScaler.inverse_transform(predictionsStd)\n",
    "predictionsStd_clean = []\n",
    "for p in predictionsStd:\n",
    "    predictionsStd_clean.extend(p)\n",
    "predictionsStdSeries = pd.Series(predictionsStd_clean, index = df['2017'].index)\n",
    "\n",
    "plotSeries([df['PETR4_Close_trend']+ df['PETR4_Close_resid'], \n",
    "#            df['PETR4_Close_trend'],\n",
    "            df['PETR4_Close_trend'] + predictionsStdSeries],\n",
    "           initialPlotDate = '2017-05', finalPlotDate = '2017-06',\n",
    "           title = 'Original Data vs Predicted', ylabel = 'Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSeries([df['PETR4_Close_resid'], (df['PETR4_Close_resid'] - predictionsStdSeries)],\n",
    "           plotZeroLine = True,\n",
    "           initialPlotDate = '2017-05', finalPlotDate = '2017-06',\n",
    "           title = 'Prediction Error', ylabel = 'BRL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Retraining MLPs using only the delayed series as input, no auxiliar series, for comparative purposes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsToUse = ['PETR4_Close_resid']\n",
    "\n",
    "xSimpleTrain, ySimpleTrain, xSimpleTest, ySimpleTest = prepData(df = df, \n",
    "                                                                columnsToUse = columnsToUse, columnToPredict = 'PETR4_Close_resid', \n",
    "                                                                nDelays = 10, testSetSize = len(df['2017']))\n",
    "\n",
    "xSimpleScaler = MinMaxScaler()\n",
    "xSimpleScaler.fit(xSimpleTrain)\n",
    "\n",
    "ySimpleScaler = MinMaxScaler()\n",
    "ySimpleScaler.fit(ySimpleTrain)\n",
    "\n",
    "xSimpleTrain = xSimpleScaler.transform(xSimpleTrain)\n",
    "xSimpleTest  = xSimpleScaler.transform(xSimpleTest)\n",
    "\n",
    "ySimpleTrain = ySimpleScaler.transform(ySimpleTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = xSimpleTrain.shape[1]\n",
    "\n",
    "nFolds = 10 # number of times a topology will be trained\n",
    "maxNeuronsInHiddenLayer = 10 # upper limit for number of neurons in hidden layer\n",
    "\n",
    "modelSimple = Sequential([Dense(nNeurons, activation = 'tanh', input_dim = inputDim),\n",
    "                          Dense(1, activation = 'linear')\n",
    "                         ])\n",
    "modelSimple.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['mae'])\n",
    "\n",
    "print 'começou'\n",
    "fitHistory = modelSimple.fit(xSimpleTrain, \n",
    "                             ySimpleTrain, \n",
    "                             epochs = 500,\n",
    "                             verbose = 0,\n",
    "                             shuffle = True,\n",
    "                             validation_split = 0.15, \n",
    "                             callbacks = [modelCheckpointStd,\n",
    "                                          earlyStopping]\n",
    "                            )\n",
    "print 'terminou'\n",
    "print min(fitHistory.history['val_loss'])\n",
    "if min(fitHistory.history['val_loss']) < bestSimpleValLoss:\n",
    "    print 'entrou'\n",
    "    bestSimpleValLoss = min(fitHistory.history['val_loss'])\n",
    "    bestSimpleFitHistory = fitHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCheckpointStd = ModelCheckpoint('./Models/PETR4_MLP_simple.h5', save_best_only=True)\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "bestSimpleValLoss = np.Inf\n",
    "bestSimpleFitHistory = None\n",
    "\n",
    "for nNeurons in range(1, maxNeuronsInHiddenLayer + 1):\n",
    "    for fold in range(nFolds):\n",
    "        modelSimple = Sequential([Dense(nNeurons, activation = 'tanh', input_dim = inputDim),\n",
    "                                  Dense(1, activation = 'linear')\n",
    "                                 ])\n",
    "        modelSimple.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['mae'])\n",
    "\n",
    "        print 'começou'\n",
    "        fitHistory = modelSimple.fit(xSimpleTrain, \n",
    "                                     ySimpleTrain, \n",
    "                                     epochs = 500,\n",
    "                                     verbose = 0,\n",
    "                                     shuffle = True,\n",
    "                                     validation_split = 0.15, \n",
    "                                     callbacks = [modelCheckpointStd,\n",
    "                                                  earlyStopping]\n",
    "                                    )\n",
    "        print 'terminou'\n",
    "        \n",
    "        print min(fitHistory.history['val_loss'])\n",
    "        if min(fitHistory.history['val_loss']) < bestSimpleValLoss:\n",
    "            print 'entrou'\n",
    "            bestSimpleValLoss = min(fitHistory.history['val_loss'])\n",
    "            bestSimpleFitHistory = fitHistory\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print 'Time to fit all models: '+str(end_time-init_time)+' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bestModelSimple = load_model('./Models/PETR4_MLP_simple.h5')\n",
    "bestModelSimple.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10), nrows = 1, ncols = 1)\n",
    "ax.set_title('RMSE per epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('RMSE')\n",
    "trainigSet, = ax.plot(np.sqrt(bestSimpleFitHistory.history['loss']), 'b', label = 'trainig set')\n",
    "validationSet, = ax.plot(np.sqrt(bestSimpleFitHistory.history['val_loss']), 'r', label = 'validation set')\n",
    "plt.legend(handles=[trainigSet, validationSet], labels=['training set', 'validation set'], prop={'size': 15})\n",
    "plt.figtext(0.5,  0.010, 'Lowest Validation RMSE: {}'.format(min(bestSimpleFitHistory.history['val_loss'])), size = 14, horizontalalignment = 'center')\n",
    "fig.savefig('{}/{}.{}'.format(saveFigDir, 'PETR4_MLP_simple', 'pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsSimple = bestModelSimple.predict(xSimpleTest)\n",
    "predictionsSimple = ySimpleScaler.inverse_transform(predictionsSimple)\n",
    "predictionsSimple_clean = []\n",
    "for p in predictionsSimple:\n",
    "    predictionsSimple_clean.extend(p)\n",
    "predictionsSimpleSeries = pd.Series(predictionsSimple_clean, index = df['2017'].index)\n",
    "\n",
    "plotSeries([df['PETR4_Close_trend']+ df['PETR4_Close_resid'], \n",
    "#            df['PETR4_Close_trend'],\n",
    "            df['PETR4_Close_trend'] + predictionsSimpleSeries],\n",
    "           initialPlotDate = '2017-05', finalPlotDate = '2017-06',\n",
    "           title = 'Original Data vs Predicted', ylabel = 'Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSeries([df['PETR4_Close_resid'], (df['PETR4_Close_resid'] - predictionsSimpleSeries)],\n",
    "           plotZeroLine = True,\n",
    "           initialPlotDate = '2017-05', finalPlotDate = '2017-06',\n",
    "           title = 'Prediction Error', ylabel = 'BRL')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
